# "LLM 없는 MCP 기반 에이전트" 구조 검토 및 발전 방안 제시 보고서: OpenManager Vibe V4

`openmanager-vibe-v4` 프로젝트에 현재 구현되어 있는 "LLM(거대 언어 모델) 없는 MCP(최소 컨텍스트 처리) 기반 에이전트"의 구조를 검토하고, 향후 발전 방안을 제시합니다.

## 1. 현존하는 "LLM 없는 AI 에이전트" 구성 요소 재확인

본 프로젝트에는 이미 여러 계층에 걸쳐 LLM 없이도 AI 에이전트와 유사한 기능을 수행하려는 시도가 구현되어 있습니다. 이는 실제 LLM 사용 대비 비용 및 복잡성 없이도 지능적인 사용자 경험을 제공하려는 노력으로 해석됩니다.

*   **단순 MCP 서버 (`mcp-lite-server/server.js`의 `/query` 엔드포인트):**
    *   가장 기본적인 형태로, `context/` 디렉토리 내 텍스트 파일에 정의된 내용을 기반으로 사용자 질문에 포함된 키워드와 매칭되는 줄을 찾아 응답합니다.
*   **백엔드 AI 에이전트 (`mcp-lite-server/ai_agent.js`):**
    *   수집된 서버 메트릭 데이터(예: CPU, 메모리 사용률)에 대해 Z-score와 같은 통계적 분석을 수행하여 이상 징후를 감지합니다.
    *   이를 바탕으로 간단한 패턴 기반의 자연어 응답을 생성하며, 주로 `/api/ai/query` 엔드포인트를 통해 호출됩니다.
*   **프론트엔드 AI 프로세서 (`frontend/ai_processor.js`):**
    *   현재 시스템에서 가장 정교하고 복잡한 "LLM 없는 AI 에이전트" 로직을 담고 있습니다.
    *   상세한 규칙 기반의 문제 패턴(`problemPatterns`)을 다수 정의하고 있으며, 이를 통해 서버의 다양한 상태를 진단합니다.
    *   사용자 질의를 분석(`analyzeQuery`)하여 의도를 파악하고, 적절한 정보(원인 분석, 해결책 제시, 관련 데이터 조회 등)를 생성하거나 보고서 형태로 가공하여 제공하는 기능을 프론트엔드에서 직접 수행합니다.

## 2. 각 구성 요소의 역할 및 현재 수준 요약

각 "LLM 없는 AI 에이전트" 구성 요소는 다음과 같은 역할과 현재 수준을 가지고 있습니다.

*   **단순 MCP 서버 (`/query`):**
    *   **역할:** 사전 정의된 컨텍스트 파일의 내용을 기반으로 하는 기초적인 정보 제공자입니다.
    *   **현재 수준:** 사용자 질문 내 단어가 컨텍스트 파일의 특정 줄에 포함되어 있는지 여부만을 확인하는 단순 문자열 매칭 방식입니다. 문맥 이해나 복잡한 조건 처리는 불가능합니다.
*   **백엔드 AI 에이전트 (`ai_agent.js`):**
    *   **역할:** 실시간 또는 주기적으로 수집되는 서버 메트릭 데이터에 대한 통계적 분석을 통해 "비정상" 상태를 감지하는 역할을 합니다.
    *   **현재 수준:** Z-score를 활용하여 특정 메트릭의 이상치를 판단하고, 매우 제한된 유형의 질문(예: "CPU 높은 서버는?")에 대해 고정된 형태의 응답을 생성합니다.
*   **프론트엔드 AI 프로세서 (`ai_processor.js`):**
    *   **역할:** 사용자 인터페이스와 가장 가까운 곳에서, 복잡한 조건과 로직을 사용하여 서버 상태를 다각도로 분석하고, 사용자의 다양한 질문 의도를 파악하여 상세한 정보와 해결책을 제공하려는 시도를 합니다.
    *   **현재 수준:** 현재 시스템의 "지능" 대부분이 이 구성 요소에 집중되어 있습니다. 수십 개의 문제 패턴과 상세한 질의 분석 로직을 통해, LLM 없이도 상당히 구체적이고 유용한 응답을 생성하려 노력하고 있습니다. 하지만 모든 로직이 프론트엔드에 있어 확장성, 유지보수성, 성능 면에서 한계가 있을 수 있습니다.

## 3. `frontend/ai_processor.js`의 핵심 역할 조명

`frontend/ai_processor.js` 파일은 현재 `openmanager-vibe-v4` 시스템의 "LLM 없는 지능"을 구현하는 데 있어 중추적인 역할을 담당하고 있습니다. 이 파일 내의 다음과 같은 요소들이 핵심입니다:

*   **`problemPatterns` 배열:** 이 배열에는 다양한 서버 문제 상황(예: CPU 임계치 초과, 메모리 부족, 서비스 중단, 특정 오류 메시지 발생 등)에 대한 상세한 정의가 포함되어 있습니다. 각 패턴은 문제 발생 조건, 설명, 심각도, 가능한 원인, 그리고 권장 해결책까지 명시하고 있어, 규칙 기반 전문가 시스템과 유사한 형태로 동작합니다.
*   **`analyzeQuery(query)` 메서드:** 사용자로부터 입력받은 자연어 질문(`query`)을 분석하여 주요 키워드, 요청 유형(일반 조회, 문제 분석, 해결책 요청 등), 대상 메트릭, 임계값 등을 추출하려는 시도를 합니다. 이를 통해 사용자의 의도를 파악하고 적절한 다음 액션을 결정합니다.
*   **다양한 `generate...Response` 및 `generate...Report` 함수들:** `analyzeQuery`를 통해 분석된 사용자의 의도와 `problemPatterns`를 통해 진단된 서버 상태를 종합하여, 사용자에게 제공할 최종 응답 메시지나 보고서를 생성합니다.

이처럼 `frontend/ai_processor.js`는 사전에 정의된 수많은 규칙과 패턴의 조합을 통해, 마치 AI가 분석하고 응답하는 것과 유사한 사용자 경험을 제공하려 하고 있습니다. 이는 LLM의 유연성이나 문맥 이해 능력에는 미치지 못하지만, 특정 도메인(서버 모니터링)에 특화된 지식을 효과적으로 활용하는 방식이라 할 수 있습니다.

## 4. 발전 방안 제안

현재의 "LLM 없는 MCP 기반 에이전트" 구조를 한 단계 발전시키고, 보다 효율적이고 확장 가능한 시스템을 구축하기 위해 다음과 같은 방안들을 제안합니다.

### 4.1. 컨텍스트 파일 구조화

현재 `mcp-lite-server/context/` 내 `.txt` 파일은 단순 텍스트 라인으로 구성되어 있어, 정보의 구조적 표현 및 추출이 매우 제한적입니다. 이를 개선하기 위해 **JSON 또는 YAML 형식으로 변경**할 것을 제안합니다.

*   **장점:**
    *   질문 키워드, 예상되는 사용자 의도, 응답 템플릿, 다음 액션(API 호출, 추가 질문 유도 등), 관련 데이터 요청 방식 등을 구조적으로 명시할 수 있습니다.
    *   파싱 및 정보 추출이 용이해지며, 컨텍스트 관리의 효율성이 증대됩니다.
    *   보다 복잡하고 다양한 상호작용 시나리오를 정의할 수 있습니다.

*   **샘플 (YAML 예시):**
    ```yaml
    - id: "cpu_high_usage_check"
      description: "CPU 사용률 과다 문제 확인"
      keywords: ["cpu", "사용률", "높음", "과부하", "문제", "느림"]
      intent: "check_cpu_problem"
      response_template: "CPU 사용률과 관련된 문제를 분석하시겠습니까? 현재 CPU 사용량이 높은 상위 서버 목록을 보여드릴까요?"
      actions:
        - type: "suggest_api_call"
          message: "CPU 높은 서버 목록 보기"
          api_endpoint: "/api/servers/metrics/cpu_high" # 백엔드에 해당 API 구현 필요
        - type: "suggest_query"
          display_text: "CPU 높은 서버 해결 방법은?"
          query_text: "cpu 문제 해결 방법"
      required_context_data: # 이 컨텍스트를 처리하기 위해 필요한 추가 데이터
        - "server_list"
        - "current_cpu_metrics"

    - id: "cpu_high_usage_solution"
      description: "CPU 사용률 과다 문제 해결책 제시"
      keywords: ["cpu", "해결", "방법", "조치"]
      intent: "get_solution_cpu_problem"
      response_template: "CPU 사용률이 높은 경우, 일반적인 원인으로는 {{causes}} 등이 있으며, {{solutions}} 등의 조치를 시도해볼 수 있습니다. 상세 분석을 위해 '{{hostname}} 서버 CPU 상세 분석'과 같이 질문해주세요."
      # 이 경우, causes와 solutions는 백엔드 로직(problemPatterns)에서 가져올 수 있음
    ```

### 4.2. 매칭 로직 개선 (MCP 서버)

`mcp-lite-server`의 단순 포함 검색 로직(`line.includes(word)`)을 개선하여 사용자 질의와 컨텍스트 간의 관련성 평가 정확도를 높입니다.

*   **TF-IDF (Term Frequency-Inverse Document Frequency) 활용:**
    *   Node.js 환경에서 `natural` 라이브러리의 TF-IDF 모듈 등을 사용하여 사용자 질의와 각 컨텍스트 문서(또는 구조화된 컨텍스트 내 특정 필드) 간의 관련성을 수치적으로 계산할 수 있습니다.
    *   이를 통해 단순 키워드 포함 여부보다 더 정교하게 관련 있는 컨텍스트를 찾아낼 수 있습니다.
*   **키워드 가중치 부여 및 다중 키워드 매칭 강화:**
    *   질문에서 중요한 키워드(예: "CPU", "장애")와 부수적인 키워드를 구분하여 가중치를 다르게 적용하거나, 여러 핵심 키워드가 함께 등장하는 컨텍스트에 더 높은 우선순위를 부여하는 로직을 구현합니다.

### 4.3. 프론트엔드-백엔드 역할 분담 명확화 및 백엔드 기능 강화

현재 `frontend/ai_processor.js`에 과도하게 집중된 복잡한 분석 로직, 문제 패턴 정의(`problemPatterns`), 질의 분석 로직(`analyzeQuery`) 등을 **백엔드 API로 이전**하는 것을 강력히 권장합니다.

*   **기대 효과:**
    *   **유지보수 용이성 증대:** 핵심 로직 변경 시 프론트엔드 재배포 없이 백엔드만 업데이트하면 됩니다.
    *   **일관성 있는 로직 제공:** 향후 다른 클라이언트(예: 모바일 앱, 챗봇 인터페이스)가 추가되더라도 동일한 백엔드 로직을 재사용하여 일관된 사용자 경험을 제공할 수 있습니다.
    *   **프론트엔드 경량화:** 프론트엔드는 사용자 인터랙션 및 결과 표시에 집중하게 되어 코드 복잡도가 낮아지고 성능이 개선될 수 있습니다.
    *   **보안 강화:** 민감할 수 있는 규칙이나 로직이 클라이언트 코드에 직접 노출되지 않습니다.
    *   **확장성 향상:** 백엔드에서는 더 복잡한 데이터 처리, 외부 시스템(모니터링 도구, 알림 시스템 등)과의 연동, 데이터베이스 활용 등이 훨씬 용이합니다.

*   **구현 방안 예시:**
    1.  `problemPatterns` 정의를 백엔드로 옮겨 DB나 설정 파일 형태로 관리합니다.
    2.  `analyzeQuery`와 유사한 사용자 질의 분석기(NLU 모듈, 아래 4.4 참조)를 백엔드에 구현합니다.
    3.  백엔드 API는 분석된 질의와 `problemPatterns`를 바탕으로 진단 결과를 생성하고, 필요한 경우 연관 메트릭 데이터와 함께 구조화된 형태로 프론트엔드에 전달합니다.
    4.  프론트엔드는 이 구조화된 데이터를 받아 사용자에게 적절히 시각화하고 상호작용을 처리합니다.

### 4.4. 점진적인 자연어 이해(NLU) 기능 도입

완전한 LLM을 도입하는 것은 현재 프로젝트의 "LLM 없는" 기조와 맞지 않을 수 있지만, 사용자 질문 이해도를 높이기 위해 경량 NLU 기능을 점진적으로 도입하는 것을 고려할 수 있습니다.

*   **의도 분류 (Intent Classification):** 사용자의 질문이 "상태 조회", "문제 원인 분석", "해결책 요청", "보고서 생성" 등 어떤 의도를 가지고 있는지 분류합니다.
*   **주요 개체명 인식 (Named Entity Recognition - NER):** 질문에서 서버 이름, 메트릭 종류(CPU, 메모리), 시간 표현 등 중요한 정보를 추출합니다.
*   **활용 가능한 라이브러리:**
    *   **Node.js:** `natural` (기본적인 분류, 토큰화, TF-IDF 등 제공), `compromise` (간단한 NLP 처리)
    *   **Python API 연동:** 만약 Python 사용이 가능하다면 `spaCy`나 `NLTK`와 같은 강력한 라이브러리를 사용하여 NLU API를 구축하고, Node.js 백엔드에서 이를 호출하는 방식도 고려할 수 있습니다. (이 경우 별도의 Python API 서버 운영 필요)

## 5. 샘플 코드 또는 개념 설명 (재강조)

*   **컨텍스트 파일 구조화 예시:** 위 4.1 항목에서 제시된 YAML 형식은 컨텍스트를 어떻게 더 유용하게 만들 수 있는지 보여줍니다. 각 항목은 `id`, `description`, `keywords`, `intent`, `response_template`, `actions` 등으로 구성되어, 단순 텍스트보다 훨씬 풍부한 정보를 담고 상호작용을 유도할 수 있습니다.
*   **`frontend/ai_processor.js` 로직의 백엔드 이전:**
    *   `frontend/ai_processor.js`의 `analyzeQuery` 메서드는 사용자의 자연어 입력을 분석하여 어떤 정보를 원하는지, 어떤 조건(예: CPU 사용률 80% 이상)에 관심 있는지 등을 파악하려는 로직입니다. 이 로직이 백엔드로 이동하면, 프론트엔드는 단순히 사용자 입력을 백엔드 API로 전달하고, 백엔드가 이 분석을 수행하여 "사용자는 CPU 사용률이 80% 이상인 웹 서버 목록을 원함"과 같은 구조화된 분석 결과를 도출합니다.
    *   마찬가지로, `problemPatterns` 배열과 이를 순회하며 서버 상태를 진단하는 로직도 백엔드에서 수행됩니다. 백엔드 API는 이 진단 결과를 바탕으로 "server-01에서 critical_cpu 문제 발생"과 같은 정보를 생성하고, 필요시 관련 원인/해결책 정보와 함께 프론트엔드에 전달합니다. 프론트엔드는 이 정보를 받아 사용자에게 적절한 형태로 보여주는 역할에 집중합니다.

이러한 발전 방안들을 통해 `openmanager-vibe-v4` 프로젝트의 "LLM 없는 MCP 기반 에이전트"는 현재의 장점을 유지하면서도 더욱 지능적이고, 확장 가능하며, 유지보수가 용이한 시스템으로 발전할 수 있을 것입니다.
